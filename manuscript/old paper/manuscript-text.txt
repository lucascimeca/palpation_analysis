%############# INTRODUCTION ###########
\section{Introduction}
In the last decades, substantial efforts have been made in enhancing the sensing capabilities 
of robots by providing them with a sense of touch [1]. Haptic sensing differs from other modalities, 
such as vision, in virtue of its tight coupling with, and need of, physical interactions. 
Haptic sensing requires direct physical contacts with sensing targets, inducing spatiotemporal 
force patterns on the contact surface, which may or may not be the consequence of motor behaviors 
of the robots. Furthermore, force patterns are also significantly related to the shape and material 
properties (e.g. stiffness) of sensing surfaces and the target objects. 

In medical palpation diagnosis, for example, given the nature of the soft tissues in the body and 
the scarcity of helpful stimuli other than touch, haptic perception plays a fundamental role. 
Here, practitioners necessitate the use of different physical interaction strategies according to 
the palpation task, whether this is an organ to examine, finding hard (cancerous) inclusions or 
investigating their characteristics (cite paper). In these cases, contacts and physical interactions 
are the basis of rich sensory stimuli, with which practitioners can judge the conditions of target areas (CITE).

The influences of active embodied interactions for sensing tasks have been intensively 
investigated in the past years. To different extents, for a theory of sensory-motor coordination, research 
agrees on the need of active motor-coordinated behaviors, the desirability of a dimensionality reduction 
process and the necessity of a process capable of differentiating between different stimuli.

% SMC

First, active motor-coordinated interaction can directly select or influence the perceived sensory patterns.
The consequences of motion to the perceived stimuli have been the focus of much
work in the past years, where research has shown how said influence can actively improve
the ability to retrieve information from the world and induce structure for simplified information processing
(information self-structuring). However, accounting for motor interaction induces the rich spatiotemporal 
information retrieved to be ofter redundant and highly dimensional.

% dimensionality reduction & Cognitive map

Second, dimensionality reduction processes are necessary to pass from a set of  highly dimensional sensor 
information to a subset of invariant properties of the sensory signal. These invariances, should both 
fundamentally describe the stimulus perceived, and differentiate it from other, possibly similar, stimuli. 
In this paper, we refer to a cognitive map as a mathematical, minimalistic, representation method of sensory 
stimuli. 

% Categorization and Category formation

And third, the structure of sensory stimuli generated from the interaction with different objects, helps to 
understand similarities or differences of the objects themselves. Through pertinent physical interactions, 
sensory stimuli of similar objects will maintain strong invariant similarities in the sensing space, whilst 
increasing their difference with dissimilar objects.

Categorization follows from the separation in the sensor space of the structured stimuli. Through suitably 
structured information, then, the invariances allow for the dissociation of stimuli originated from different 
objects and the association, instead, of stimuli derived from similar objects.

% Problem statement
Despite the afore mentioned progresses, the described components are related, highly complex and dynamic 
processes. In particular, in some cases, due to the conformity of the sensor response under specific 
circumstances, it is impossible to categorize stimuli appropriately, without first duly exploiting physical 
interactions. Moreover, the influence of motor-coordinated interactions has mainly been analyzed singularly 
or only with respect to particular task to solve. 

%hypothesis
In this context, it is worthwhile understanding whether motor-coordinated interactions can appropriately influence 
the sensor response, introducing structure to facilitate the categorization of stimuli which would be impossible 
to otherwise categorize. In addition, it is fundamental to assess whether these effects are simply dependent or 
the task to solve, or if the effects are somehow influenced but the properties of the sample stimuli retrieved 
from interactions in the world. Lastly, there is a need to inquire over the possibility of extracting additional 
information from cognitive maps, i.e. uncovering similarity associations between categories of stimuli.

% contribution
This paper proposes an approach to obtain cognitive maps for the categorization of tactile stimuli. We explore 
the way motor induced physical interaction with a soft body affects the structure of haptic spatiotemporal 
information. Moreover, we show how the influence of motor-coordinated interactions to the structuring of the 
information can not be analyzed only with respect to the task to solve, but that its effect primarily depends 
on the sample stimuli retrieved from interactions in the world. 


% structure of this paper
The paper is organized as follows: In Section \ref{sec_methods} we describe the methods of the paper, starting 
from the overall approach in section \ref{sec_overall_approach}, and continuing to how we acquire the tactile data, 
perform dimensionality reduction and categorize the tactile stimuli in sections \ref{sec_skin}, 
\ref{sec_dim_reduction} and \ref{sec_clustering} respectively. We describe the motor-coordinated interactions 
in section \ref{sec_motor_interactions}, and the set up of the experiments in section \ref{sec_experimental_setup}. 
In section \ref{sec_results} we report the results of the experiments followed by discussion and conclusion 
in sections \ref{sec_discussion} and \ref{sec_conclusion}.

%############# METHODS###########
\section{Methods} \label{sec_methods}

\subsection{Overall Approach} \label{sec_overall_approach}

To actualize the framework described we use an unsupervised clustering method to enable an agent to 
automatically categorize oncoming stimuli. A robot interacts with a soft body through a
sensorised probe. By changing the motion strategy of the robot during the interactions with the soft body, 
the sensory response of the sensorised probe changes, modifying the retrieved data
and consequently the discretization of the stimuli into categories (Fig. 1). 

The choice of the number of categories to divide the space into is also an important influencing 
factor on the resulting categorization. By analyzing which motion influences the sensor response
coherently with the finding of meaningful categories for the task at hand, we can assess
the relationship between motion and the underlying change in the robot's cluster
perception. The use of an unsupervised learning technique is here fundamental, as it
allows the robot to base the inference solely on the information structured induced by the
motion strategy, and without influencing it in any active way.

FIGURE 1 EXPLAINING CONCEPTUAL FRAMEWORK FOR THE EXPERIMENTS IN THIS PAPER

For the experiments, we develop a sensorised probe provided with a new capacitive
tactile sensor array with high spatial resolution [6], we design a soft, elastic, phantom
organ and set two qualitatively different types of probing strategies. During interaction 
with the soft phantom organ, the different types of motion utilized changes the sensor response. 
The altered sensory input directly influences the way in which the probed locations in the 
phantom are perceived and in turn their clustering. We simplify the scenario by choosing hard
nodules of two different sizes and embed them in the phantom organs at two depths.
We wish to observe how when fixing the way we categorize oncoming stimuli, we can affect 
the robot's its internal representations of the object interacted with by changing the 
interaction itself probing motion.



\subsection{Tactile Sensor Technology and Data Acquisition} \label{sec_skin}

The tactile sensor technology used in the experiments has been described in \cite{schmitz_methods_2011}. 
The adopted sensing mode is based on the capacitive transduction principle. 
A capacitive transducer (i.e., a tactile element, or $taxel$) is organized in a 
layered structure: the lower layer consists of the positive electrode, which is mounted 
on a Flexible Printed Circuit Board (FPCB); a small air chamber act as dielectric and the 
upper layer is a ground plane made with conductive lycra. The tactile sensor is made up 
of a number of taxels geometrically organized in triangular modules (Fig. \ref{CySkin:skin}). 

In the current prototype, each module hosts $7$ taxels, as well as the Capacitance to Digital 
Converter (CDC) chip (namely, the AD7147 from Analog Devices) for converting capacitance 
values to digital. The CDC chip can measure variations in capacitance values with 16 bits 
of resolution. All the modules are interconnected and communicate through an SPI bus to a 
read-out board which perform a preliminary processing of the tactile sensor data and send 
them to the PC through CAN bus (Fig. \ref{CySkin:schema}). %within the $4 \div 30pF$ range 
with a sensitivity of $0.32fF$. 

FIGURE 2 EXPLAINING SENSOR

In this context, the normal forces exerted on the sensor produce variations in capacitance 
values reflecting the varied pressure over the taxel positions. A sensor reading, or tactile 
image, from the tactile sensor described is produced at $20Hz$, and corresponds to 
a 7-dimensional array, where each element contains the capacitance variation value of 
the corresponding taxel (Fig. \ref{CySkin:skin}).

%is $\Delta C_i$, and $i$ is its corresponding taxel in the sensor.
When performing the experiments, the  tactile images are not used singularly. For every 
motion strategy designed, we take sensor readings in sequence over equally spaced intervals, 
varying with the motion strategy, and concatenate them sequentially into a single array 
(time series). In this paper we refer to a tactile image sequence as the concatenated sensor 
readings after applying a specific motion strategy.

To carry out the experiments we design and 3d-print a set of hard inclusions to embed in the 
soft phantom.


\subsection{Dimensionality reduction}\label{sec_dim_reduction}

As previously mentioned, a process is needed to reduce the high dimensionality of the spatiotemporal
data acquired to the tactile sensor, while interacting with the world.

After acquiring tactile image sequences for each probed location like mentioned in section \ref{sec_setup}, 
We use Principal Component Analysis projection ($PCA$) \cite{tipping_probabilistic_1999} to reduce 
the dimensionality of the acquired data \cite{lloyd_least_1982}. 

We start the process with tactile image sequences for each probed location in the phantom. The details 
of the image sequences capture are irrelevant for the generic process here described, thus we leave 
their explanation to later sections. For a set of $N$ different locations, let $\mathbf{X}$ be 
a $(N\times D)$ matrix where each unique tactile image sequence for a probed location is a $D$  
dimensional row ($D\gg2$) in the matrix. As previously mentioned, we define a tactile image sequence 
as a sequence of sensor readings, each of which is sequentially concatenated into a single array. 
The dimension of $D$, then, will be strictly dependent on the motion strategy and on the space 
interval at which it is decided to capture each tactile image within the time sequence. 

After obtaining the tactile image sequences matrix $\mathbf{X}$, we begin the process be finding 
the average tactile sequence $\vec{\mu}$ as:
\begin{equation}
\vec{\mu} = \frac{1}{n}\sum_{i=1}^{n}\vec{x}_i
\end{equation}
where $\vec{x}_i$ is a column vector corresponding to the $i^{th}$ row in $\mathbf{X}$. 

We proceed by computing the scatter matrix of $\mathbf{X}$ as
\begin{equation}
\mathbf{S} = \sum_{i=1}^{n}(\vec{x}_i-\vec{\mu})(\vec{x}_i-\vec{\mu})^T
\end{equation}

We use Single Value Decomposition to factorize $\mathbf{S}$ into
\begin{equation}
\mathbf{S} = \mathbf{Q}\mathbf{\Lambda} \mathbf{Q}^{-1}
\end{equation}
\noindent where $\mathbf{Q}$ is matrix such that each column $q_j$ corresponds to an eigenvector 
of $\mathbf{S}$, and each element $\lambda_{jj}$ in the diagonal matrix $\Lambda$ is its corresponding 
eigenvalue. 

We list the eigenvectors in ascending order of eigenvalue and select the first two in the list. 
Let $\vec{p}_1$ and $\vec{p}_2$ be the selected eigenvectors obtained from $PCA$. 

We form a $(D\times 2)$ projection matrix $\mathbf{P}$ as:
\begin{equation}
\mathbf{P}=\begin{bmatrix}\vec{p}_1^{\ T}, \vec{p}_2^{\ T}\end{bmatrix}	
\end{equation}
where $\vec{p}_1^{\ T}$ and $\vec{p}_2^{\ T}$ are column vectors in $\mathbf{P}$. 

Finally, we project the $D$-dimensional row vectors in $\mathbf{X}$ onto a 2-dimensional subspace by:
\begin{equation}
\mathbf{W}=\mathbf{X}\cdot \mathbf{P}
\end{equation}
where $\mathbf{W}$ is a $(N\times 2)$ matrix and each row in it is a 2-dimensional $encoding$ of a 
tactile image sequence for a probed location.


\subsection{Category Formation}\label{sec_clustering}

To observe the effects of motion to in sensory-motor coordination framework we wish to have a process to categorize
stimuli. We use K-Means Clustering ($KMC$) to find clusters in the stimuli. We use the algorithm with a random 
centroid initialization, and split the re-encoded sequences in $\mathbf{W}$ into $k$ clusters, thus:
\begin{equation}
\vec{v} = KMC_{k}(\mathbf{W})
\end{equation}
where $\vec{v}$ is an N-dimensional array, $\forall i\in \{1, 2,\ ...\ , N\}$,   $\vec{v}_i\in\{0, ..., k\}$, 
and $\forall i\ \exists j.\ i\neq j\ \land\ v_i\neq v_j$ (no one cluster can contain all objects). 

In general $\vec{v}_i=0\ iff$ the $i^{th}$ tactile image belongs to cluster 0, $\vec{v}_i=1\ iff$ the $i^{th}$
tactile image belongs to cluster 1 (Fig. \ref{self_org_processing}) and so forth, thus the $\vec{v}$ vector 
contains the cluster membership of each probed location in the initial set. To avoid cluster anomalies due 
to the random centroid initializations we run the K-Means Clustering algorithm three times and discard the 
clustering attempt if, after convergence, any of the three cluster guesses vectors differs from any other. 

In this context, the cluster assignments for each probed location is largely dependent on 
the motion strategy employed. The change in cluster assignment is the main object of analysis in the 
following sections.

FIGURE SHOWING FLOWCHART OF EXPERIMENTS

\subsection{Motor-coordinated Interactions} \label{sec_motor_interactions}

The probing procedure begins with the user teaching the robot the location of the nodules. This is done by manually 
moving the head to each of the sixteen test locations, taking care to carefully align the head above the centre of 
each nodule. Once this is done, the robot automatically probes each location in turn using the two motions shown in 
Figure \ref{probing}. 

We differentiate between two qualitatively different types of motion, vertial and rotatory. Both the vertical and 
rotatory motions can be performed with a set of three parameters, varied over different experiments.

First, the vertical motion is performed, where the probe is aligned vertically and plunged 
directly down in to the phantom in 0.5 mm increments. After each increment, the robot briefly pauses to allow a 
tactile image to be recorded before continuing with the next movement. This continues until the probe is at a 
depth of $d$ below the surface of the silicone, whereupon it returns to a neutral position 10 mm above the 
surface in a single movement.

Second, the rotary motion is performed when the robot rotates about a point 
$d$ mm below the surface of the silicone, keeping the sensor $r$ mm away from this nexus point. First, 
the robot moves vertically down so that it is at the correct distance from the nexus. It then rotates in the 
$+\theta$ direction until it is at an radius of 30\degree from the vertical position. From here, the probe rotates in 
the $-\theta$ direction in 1\degree increments, recording a tactile image after each step. Once the probe has 
moved through 60\degree it stops recording, rotates back to vertical then returns to the neutral position 10 mm 
above the surface of the silicone. This procedure is repeated for $r\in\lbrace 10 mm, 12 mm, 14 mm, 16 mm \rbrace$ 
and $d\in\lbrace 10 mm, 12 mm, 14 mm, 16 mm \rbrace$.

FIGURE OF THE MOTION STRATEGIES


\section{Experimental Setup} \label{sec_experimental_setups}

We build a soft phantom organ using Ecoflex 00-10\footnote[2]{https://www.smooth-on.com/products/ecoflex-00-10/} 
from Smooth-on. The phantom organ contained hard inclusions placed at two different depth $5mm$ and $15mm$, a
nd having two different diameters namely $7mm$ and $20mm$. Hereafter we may refer to a $5mm$ inclusion placed 
at a depth of $7mm$ as `$ss$', a $15mm$ inclusion placed at $7mm$ as `$bs$', a $5mm$ inclusion placed at $20mm$ 
as `$sd$' and a $15mm$ inclusion placed at $20mm$ as `$bd$'.
The hard nodules have been designed and printed with a 3D printer.
Figure \ref{phantom} shows the position and dimension of the hard nodules and how 

We 3D-print a custom-made end-effector onto which we integrate a capacitive tactile sensor onto its surface 
to retrieve $tactile\ images$ during the probing experiments.

FIGURE OF THE PHANTOM POSITIONAL SHEMATA

FIGURE OF THE PHANTOMS USED IN THE EXPERIMENTS



The printed end-effector, coupled with the tactile sensor, is mounted onto an ST-Robotics R12/5 robotic 
arm\footnote[3]{http://www.robotshop.com/uk/st-robotics-r12-5-axis-articulated-robot-arm.html}.  
(Fig. \ref{setup}). We carry out the experiments by probing the chosen phantom as described in section 
\ref{sec_motor_interactions}, while retrieving the tactile sensor data like previously described. 
After the probing has ended, the time-concatenated data is used to form the $\mathbf{X}$ tactile image 
sequence matrix described in Section \ref{sec_dim_reduction}. The data for the palpation experiment
can then be processed as described in sections \ref{sec_dim_reduction} and \ref{sec_clustering}, 
obtaining cluster memberships for each of the probed locations in the phantom.


\section{Results} \label{sec_results}

To assess the performance of the unsupervised clustering method we first need to match the clusters to 
the true target classes for the phantom under analysis. We use a cluster matching process based on maximal 
accuracy i.e.:
\begin{equation}
\vec{v}^{\ \prime} = \mathbf{CM}(\vec{v},\ \vec{t})
\end{equation}
Given the targets $\vec{t}$ and a cluster guess vector $\vec{v}$ then, $\vec{v}\ '$ is a new vector such that 
\begin{flalign*}
&\forall i \in \{1,2, ... , N\}.\\
&\  (\vec{v}_i =1 \implies \vec{v}_i^{\ \prime}=0) \land  (\vec{v}_i =0 \implies \vec{v}_i^{\ \prime}=1)\\
&\iff\ ||\vec{v}-\vec{t}||>||\vec{v}_i^{\ \prime}-\vec{t}||
\end{flalign*} 
Essentially, we associate a cluster guess to a target cluster maximizing accuracy for the particular 
task (Fig. \ref{cluster_matching}). A vector $\vec{v}=[0\ 0\ 0\ 1]$ for a task $\vec{t}_k=[1\ 1\ 1\ 0]$, 
for example, would be re-associated as $\vec{v}^{\ \prime}=[1\ 1\ 1\ 0]$. We utilize the process to benchmark the 
performance of the algorithm after palpation.




% ######## for discussion section later ####
% 
% Moreover, 
% we show how the influence of motor-coordinated interactions to the structuring of the information can not be analyzed singularly, 
% as its effect primarily depend on the samples and therefore on the nature of the information itself and the quality and quantity 
% of the data at hand. 